---
title: "GroupA_HW3"
output: html_document
date: "2023-11-20"
---

# FSDS - Chapter 6

### Exercise 6.12
For the *UN* data file at the book’s website (see Exercise 1.24), construct a multiple regrSSEion
model predicting Internet using all the other variables. Use the concept of multicollinearity
to explain why adjusted $R^2$ is not dramatically greater than when GDP is the sole predictor.
Compare the estimated GDP effect in the bivariate model and the multiple regrSSEion model
and explain why it is so much weaker in the multiple regrSSEion model.

*Solution*

```{r}
UN.data <- read.table("UN.txt", header=T)
```

```{r}
full.model <- lm(Internet ~ GDP + HDI + GII + Fertility + CO2 + Homicide + Prison, data=UN.data)
summary(full.model)
```
```{r}
GDP.model <- lm(Internet ~ GDP, data=UN.data)
summary(GDP.model)
```

```{r mSSEage=FALSE, warning=FALSE, include=FALSE}
library("olsrr")
```

```{r}
ols_vif_tol(full.model)
```

The increase on the adjusted $R^2$ is not dramatically high because it penalizes the $R^2$ value by the number of parameters. Then, only if the increase of explained variance is significative, the adjusted $R^2$ will increase dramatically.
From the Variance Inflation Factor (VIF) we can see that the covariate HDI has value greater than 10, which suggest the multicollinearity. Similarly the covariates GDP and GII have a really high value which means that most information could be obtained from the other informations.

### Exercise 6.14
The dataset *Crabs2* at the book’s website comes from a study of factors that affect sperm traits of male horseshoe crabs. A response variable, SpermTotal, is the log of the total number of sperm in an ejaculate. It has $\bar{y} = 19.3$ and $s = 2.0$. The two explanatory variables used in the R output are the horseshoe crab’s carapace width (CW, mean 18.6 cm, standard deviation 3.0 cm), which is a measure of its size, and color (1 = dark, 2 = medium, 3 = light), which is a measure of adult age, darker ones being older.
(a) Using the results shown, write the prediction equation and interpret the parameter estimates.
(b) Explain the differences in what is tested with the F statistic (i) for the overall model, (ii)
for the factor(Color) effect, (iii) for the interaction term. Interpret each.

```{r echo=FALSE, warning=FALSE}
Crabs2 <- read.table("Crabs2.txt", header=T)
summary(lm(SpermTotal ~ CW + factor(Color), data=Crabs2))
library(car)
Anova(lm(SpermTotal ~ CW + factor(Color) + CW:factor(Color), data=Crabs2))
```

*Solution*

(a) $y = \beta_0 + \beta_1x_{CW} + \beta_2x_{medium} + \beta_3x_{light}$
The increase of the carapace width leads to the increase of the amount of the sperm. Therefore the lighter the color is, the more is the increase of the amount.
(b) The analysis of variance (ANOVA) test if the difference in the means are the same. From the results we can see that the CW and the color have a small p-value, then null hypotesis is rejected and then we have an increase of the amount of the sperm on the increase of CW and Color. The interaction doesn't have a big effect in the prediction.


### Exercise 6.30
When the values of y are multiplied by a constant c, from their formulas, show that $s_y$ and $\hat\beta_1$
in the bivariate linear model are also then multiplied by c. Thus, show that $r = \hat\beta_1(s_x/s_y)$ does
not depend on the units of measurement.

*Solution*

b0 = mean(X) + b1 mean(Y)


$y = \beta_0 + \beta_1x_1 + e$ with $e$ ~ $N(0, \sigma^2X^TX)$ then $y$ ~ $N(\beta_0 + \beta_1x_1, \sigma^2X^TX)$

$cy = c(\beta_0 + \beta_1x_1 + e)$ and then $y$ ~ $N(c\beta_0 + c\beta_1x_1, c^2\sigma^2X^TX)$
or
$\rho \frac{s_{cy}}{s_x} = \rho \frac{c \cdot s_y}{s_x}$

$\hat\beta_1 = \frac{cov(x, y)}{Var(x)} = \rho \frac{c \cdot s_y}{s_x}$ then $\frac{cov(x, cy)}{Var(x)} = c \frac{cov(x, y)}{Var(x)} = c\hat\beta_1$

from this two results we obtain

$\rho = \hat\beta_1 (\frac{s_x}{s_{y^*}}) = c \hat\beta_1 (\frac{s_x}{c s_{y}}) = \hat\beta_1 (\frac{s_x}{s_y})$ with $y^* = cy$ 


### Exercise 6.42
You can fit the quadratic equation $E(Y) = \beta_0 +\beta_1x+\beta_2x^2$ by fitting a multiple regrSSEion model
with $x_1 = x$ and $x_2 = x^2$.
(a) Simulate 100 independent observations from the model $Y = 40.0−5.0x+0.5x^2 + e$, where X
has a uniform distribution over $[0, 10]$ and $ϵ ∼ N(0, 1)$. Plot the data and fit the quadratic model. Report how the fitted equation compares with the true relationship.
(b) Find the correlation between x and y and explain why it is so weak even though the plot
shows a strong relationship with a large $R^2$ value for the quadratic model.

*Solution*

```{r}
x <- sort(runif(100, 0, 10))
e <- rnorm(100)
Y <- 40 - 5*x + 0.5*x^2 + e
```

```{r}
model <- lm(Y~x + I(x^2))
summary(model)
```


```{r}
b0 <- model$coefficients[1]
b1 <- model$coefficients[2]
b2 <- model$coefficients[3]

plot(x, Y)
lines(x, b0 + b1*x + b2*x^2, col="red", lwd=2)
```

(a) With the quadratic model we have that the predicted line is similar perfectly fitting the data. The predicted parameters are very near to the real ones.

(b) The correlation is really small because the data are symmetric and then there is not a linear relationship 
```{r}
cor(x, Y)
```


### Exercise 6.52
F statistics have alternate expressions in terms of $R^2$ values.

(a) Show that for testing $H_0: \beta_1 = ⋯ = \beta_p = 0$,
$F = \frac{(TSS − SSE)/p}{SSE/[n − (p + 1)]}$ is equivalently $\frac{R^2/p}{(1 − R^2)/[n − (p + 1)]}$
Explain why larger values of $R^2$ yield larger values of F.
(b) Show that for comparing nested linear models,
$F = \frac{(SSE_0 − SSE_1)/(p_1 − p_0)}{SSE_1/[n − (p_1 + 1)]} = \frac{(R^2_1 − R^2_0)/(p_1 − p_0)}{(1 − R^2_1)/[n − (p_1 + 1)]}$

*Solution*

(a) $R^2 = \frac{RSS}{TSS} = \frac{TSS - SSE}{TSS}$

    $1 - R^2 = 1 - (1 - \frac{SSE}{TSS}) = \frac{SSE}{TSS}$
    
    $F = \frac{R^2/p}{(1 − R^2)/[n − (p + 1)]} = \frac{\frac{TSS - SSE}{TSS}/p}{(\frac{SSE}{TSS})/[n − (p + 1)]} = \frac{(TSS − SSE)/p}{SSE/[n − (p + 1)]}$
    
```{r}
p <- 12
n <- 23
TSS <- 65
SSE <- 52
R2 <- 1 - SSE/TSS

c(((TSS - SSE)/p)/(SSE/(n-p-1)), (R2/p)/((1-R2)/(n-p-1)))
```

(b) $R^2 = 1 - \frac{SSE}{TSS}$
    $1 - R^2 = 1 - (1 - \frac{SSE}{TSS}) = \frac{SSE}{TSS}$
    
    $TSS_0 = TSS_1 = TSS$ because it is a nested model
    
    $F = \frac{(R^2_1 − R^2_0)/(p_1 − p_0)}{(1 − R^2_1)/[n − (p_1 + 1)]} = \frac{((1 -\frac{SSE_1}{TSS}) - (1 -\frac{SSE_0}{TSS}))/(p_1-p_0)}{\frac{SSE_1}{TSS}/(n-p_1-1)} = \frac{(SSE_0 - SSE_1)/(p_1-p_0)}{SSE_1/(n-p_1-1)}$

```{r}
p0 <- 18
p1 <- 27
n <- 23
TSS <- 65
SSE0 <- 50
SSE1 <- 34
R21 <- 1 - SSE1/TSS
R20 <- 1 - SSE0/TSS

c( ((SSE0 - SSE1)/(p1 - p0))/(SSE1/(n-p1-1)), ((R21 - R20)/(p1 - p0))/((1-R21)/(n-p1-1)))
```


# FSDS - Chapter 7
### Exercise 7.4
Randomly sample 30 $X$ observations from a uniform in the interval $(-4,4)$ and conditional on $X = x$, 
30 normal observations with $E(Y) = 3.5x^3 − 20x^2 + 0.5x + 20$ and $σ = 30$. Fit polynomial normal GLMs of lower and higher order than that of the true relationship. Which model would you suggest? Repeat the same task for $E(Y) = 0.5x^3 − 20x^2 + 0.5x + 20$ (same $\sigma$) several times. What do you observe? Which model would you suggest now?

*Solution*

```{r}
set.seed(100)
n <- 30
x <- sort(runif(n, -4, 4))
means <- 3.5 * x^3 - 20 * x^2 + 0.5 * x + 20
y <- rnorm(n * 30, mean = rep(means, each = 30), sd = 30)
data <- data.frame(x = rep(x, each= 30), y = y)
```

```{r}
mod1 <- glm(y ~ x + I(x^2), family = gaussian, data=data)
AIC(mod1)
```
```{r}
mod2 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), family = gaussian, data=data)
AIC(mod2)
```
```{r echo=FALSE}
plot(data)
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
b2 <- mod1$coefficients[3]

lines(x, b0 + b1*x + b2*x^2, col="red", lwd=2)

b0 <- mod2$coefficients[1]
b1 <- mod2$coefficients[2]
b2 <- mod2$coefficients[3]
b3 <- mod2$coefficients[4]
b4 <- mod2$coefficients[5]

lines(x, b0 + b1*x + b2*x^2 + b3*x^3 + b4*x^4, col="green", lwd=2)
```

The red curve is the estimate from the simpler method, while the green one is from the more complex one.
Here we can see a significant change in the AIC coefficient, indicating that the second model would be more appropriate.

Let's see the other case:
```{r}
set.seed(100)
n <- 30
x <- sort(runif(n, -4, 4))
means <- .5 * x^3 - 20 * x^2 + 0.5 * x + 20
y <- rnorm(n * 30, mean = rep(means, each = 30), sd = 30)
data <- data.frame(x = rep(x, each= 30), y = y)
```

```{r}
mod1 <- glm(y ~ x + I(x^2), family = gaussian, data=data)
AIC(mod1)
```
```{r}
mod2 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), family = gaussian, data=data)
AIC(mod2)
```
```{r echo=FALSE}
plot(data)
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
b2 <- mod1$coefficients[3]

lines(x, b0 + b1*x + b2*x^2, col="red", lwd=2)

b0 <- mod2$coefficients[1]
b1 <- mod2$coefficients[2]
b2 <- mod2$coefficients[3]
b3 <- mod2$coefficients[4]
b4 <- mod2$coefficients[5]

lines(x, b0 + b1*x + b2*x^2 + b3*x^3 + b4*x^4, col="green", lwd=2)
```

Let's repeat this task several times and check how the difference between the two AIC's behave:

```{r}
m=1000
aics <- array(0, dim = m)

for (i in 1:m) {
  n <- 30
  x <- sort(runif(n, -4, 4))
  means <- .5 * x^3 - 20 * x^2 + 0.5 * x + 20
  sd <- 30
  y <- rnorm(n * 30, mean = rep(means, each = 30), sd = sd)
  data <- data.frame(x = rep(x, each= 30), y = y)
  mod1 <- glm(y ~ x + I(x^2), family = gaussian, data=data)
  mod2 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), family = gaussian, data=data)
  aics[i] <- (AIC(mod1) - AIC(mod2))/AIC(mod1)
}
hist(aics)
```

In this case the AIC coefficients and the plot indicates that the two models are very similar. Following Occam's razor principle we opt for the simpler method, i.e. the one with less parameters.

### Exercise 7.20
In the *Crabs* data file introduced in Section 7.4.2, the variable y indicates whether a female horseshoe crab has at least one satellite (1 = yes, 0 = no).
(a) Fit a main-effects logistic model using weight and categorical color as explanatory variables. Conduct a significance test for the color effect, and construct a 95% confidence interval for the weight effect.
(b) Fit the model that permits interaction between color as a factor and weight in their effects, showing the estimated effect of weight for each color. Test whether this model provides a significantly better fit.
(c) Use AIC to determine which models seem most sensible among the models with (i) interaction, (ii) main effects, (iii) weight as the sole predictor, 
(iv) color as the sole predictor, and (v) the null model.

*Solution*

(a)

```{r}
crabs <- read.table("crabs.txt", header = TRUE)
crabs$color <- as.factor(crabs$color)
model1 <- glm(y ~ weight + color, data = crabs, family = "binomial")
summary(model1)
model2 <- glm(y ~ weight , data = crabs, family = "binomial")
summary(model2)
```

We compare the residual deviances and check if the difference could have been reasonably generated by a Chi-squared distribution with 3 degrees of freedom:
```{r}
1-pchisq(195.74-188.54,3)
```

We get a borderline result that points towards the variable color not being statistically significant so we select the model with only the covariate weight.

For constructing the confidence interval for the variable weight we can use the data from the summary:

```{r}
estimate <- 1.6928
se <- 0.3888
confidence_level <- 0.95
critical_value <- qnorm((confidence_level + 1) / 2)
margin_of_error <- critical_value * se
confidence_interval <- c(estimate - margin_of_error, estimate + margin_of_error)
confidence_interval
```

Since 0 isn't inside the confidence interval we can infer that the weight variable is statistically significant for the model.

(b)

```{r include=FALSE}
library(lmtest)
```
```{r}
model_main <- glm(y ~ weight + color, data = crabs, family = "binomial")
model_interaction <- glm(y ~ weight * color, data = crabs, family = "binomial")
lr_test <- lrtest(model_main, model_interaction)
print(lr_test)
```

With a p-value of 0.07562, the evidence against the null hypothesis is not strong enough to reject it, although it's pretty close to the threshold, meaning there is no strong evidence to suggest that adding interaction terms significantly improves the model fit. 

(c)

```{r}
model_interaction <- glm(y ~ weight * color, data = crabs, family = "binomial")
model_main <- glm(y ~ weight + color, data = crabs, family = "binomial")
model_weight <- glm(y ~ weight, data = crabs, family = "binomial")
model_color <- glm(y ~ color, data = crabs, family = "binomial")
model_null <- glm(y ~ 1, data = crabs, family = "binomial")
aics <- c(AIC(model_interaction), AIC(model_main), AIC(model_weight), AIC(model_color), AIC(model_null))
aics
```

We can see that the biggest gap in AIC score is between the model having weight as covariate and the one having color. We can also see that the model having the lowest AIC score is the one taking into consideration the interaction between weight and color.

### Exercise 7.26
A headline in The Gainesville Sun (Feb. 17, 2014) proclaimed a worrisome spike in shark
attacks in the previous two years. The reported total number of shark attacks in Florida per
year from 2001 to 2013 were 33, 29, 29, 12, 17, 21, 31, 28, 19, 14, 11, 26, 23. Are these counts
consistent with a null Poisson model? Explain, and compare aspects of the Poisson model and
negative binomial model fits.

# DAAG - Chapter 8
### Exercise 6